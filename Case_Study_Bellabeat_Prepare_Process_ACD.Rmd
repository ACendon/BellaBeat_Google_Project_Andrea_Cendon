---
title: "Bellabeat_Case_Study_Google_Project"
author: "Andrea Cendón"
date: "2025-11-03"
output:
  html_document: default
---


# Google Final Project 

### Bellabeat case-study code  

#### 1. Install the packages and upload the libraries that I will use.

For this project, I will install the next packages:  

* rmarkdown -> Creates dynamic analysis documents that can have code, outputs and text.   
* skimr -> It is used for data summarization. It is necessary to upload its library.    
* tidyverse ->  Collection of packages that do data import, cleaning, transformation, visualization and a consistent and efficient analysis. It is necessary to upload its library.   
  + Libraries of tydiverse:  
    + dplyr -> It is used for data manipulation.  
    + tidyr -> Is used to clean and organize data by reshaping it into a structured format where the columns are variables and each the rows are observations.  
    + lubridate -> Allows RStudio to understand date-times and modify them.

Other libraries upload: 
 * readr ->  Allow us to import and read rectangular data (CSV) in data frames.  
 * ggplot2 ->  It is used for data visualization.  

```{r setup, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(readr)
library(lubridate)
library(skimr)
library(tidyverse)
library(ggplot2)
```


**It is important to follow all the steps in order so that the Daily_Activity_df has all the changes that you need at each point**  

#### 2. Import de data sets and check their structures.  

The complete data set download from Kaggle contains eleven files, which can be accessed through this link: <https://www.kaggle.com/datasets/arashnic/fitbit?resource=download-directory>. The names of the files are dailyActivity_merged.csv,heartrate_seconds_merged.csv,hourlyCalories_merged.csv, hourlyIntensities_merged.csv,hourlySteps_merged.csv,minuteCaloriesNarrow_merged.csv, minuteIntensitiesNarrow_merged.csv, minuteMETsNarrow_merged.csv, minuteSleep_merged.csv, minuteStepsNarrow_merged.csv, weightLogInfo_merged.csv.   

  The objective of the project is to analyze the daily behavior of the users and find trends between variables that can help to obtain new marketing strategies; because of that, I will use the data set dailyActivity_merged.csv.

##### 2.1 Main File  
  
###### 2.1.1 Import the data set using this function `read.csv()` and analyze the structure of the file.  
```{r}
Daily_Activity_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/dailyActivity_merged.csv")

#analyze the structure

head(Daily_Activity_df, 3) #This part gives me the first 3 rows and the column titles.
str(Daily_Activity_df)

```
   
   It shows 457 observations and 15 variables. It is in long format, which means that each row is an individual record and the same ID may appear in several rows.       
    
###### 2.1.2 Check if the data set has duplicates and if there are NA. 
    
```{r}
sum(duplicated(Daily_Activity_df))

# NA
skim(Daily_Activity_df)
```
  The file does not have duplicates or NA.  

###### 2.1.3 Analyze which and how many unique IDs exist.
    
```{r}
unique(Daily_Activity_df$Id) #Gives the list of IDs
print(length(unique(Daily_Activity_df$Id))) #count the number of unique IDs =35
```

  The first part of the code provides us the list of unique IDs in the whole data set. The second part counts how many unique IDs are in the data set; the result is 35 users.    

###### 2.1.4 For the final analysis I need to check how many users logged their distance manually at this point of the process.
    
```{r}
Logged_df <- Daily_Activity_df %>% 
  filter(LoggedActivitiesDistance > 0)
View(Logged_df) 

# This part is additional to this file, so that you can see the answer with out seen all the dataframe Logged_df
n_users_logged <- Logged_df %>%
  summarise(unique_users = n_distinct(Id))

n_users_logged
```
  
  The result shows that 6 users, at least one time, upload their distance manually.

##### 2.2 The other data sets 

###### 2.2.1 Import the data set using this function `read.csv()`.
```{r}
Heart_r_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/heartrate_seconds_merged.csv")
hourlyCalories_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyCalories_merged.csv")
hourlyIntensities_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlyIntensities_merged.csv")
hourlySteps_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/hourlySteps_merged.csv")
minuteCaloriesNarrow_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteCaloriesNarrow_merged.csv")
minuteIntensitiesNarrow_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteIntensitiesNarrow_merged.csv")
minuteMETsNarrow_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteMETsNarrow_merged.csv")
minuteSleep_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteSleep_merged.csv")
minuteStepsNarrow_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/minuteStepsNarrow_merged.csv")
weightLogInfo_df <- read.csv("C:/Proyecto Google/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16/weightLogInfo_merged.csv")

```

We need to import all these data sets to evaluate their structures and compare them with the main file IDs.  

###### 2.2.2 Analyze the structure of each one, if they have duplicates, the unique IDs, and how many of them exist. 
  
        2.2.2.1 Heart_r_df  

```{r}
# Analize the structure
head(Heart_r_df, 3) 
str(Heart_r_df) 

#The duplicates 
sum(duplicated(Heart_r_df)) 

# IDs
unique(Heart_r_df$Id) 
print(length(unique(Heart_r_df$Id))) 
```

The results show that this data set has only three columns with the variables ID, time, and value. Furthermore, it has zero duplicates, and the number of unique IDs is 14.  

        2.2.2.2 hourlyCalories_df  

```{r}
# Analize the structure
head(hourlyCalories_df, 3) # 3 columns, has IDs  
str(hourlyCalories_df) 

#The duplicates 
sum(duplicated(hourlyCalories_df)) #0 duplicates

# IDs
unique(hourlyCalories_df$Id) 
print(length(unique(hourlyCalories_df$Id))) #IDs = 34
```

The results show that this data set has only three columns with the variables ID, ActivityHour, and Calories. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  

        2.2.2.3 hourlyIntensities_df 
    
```{r}
# Analize the structure
head(hourlyIntensities_df, 3) # 4 columns, has IDs  
str(hourlyIntensities_df) 

#The duplicates 
sum(duplicated(hourlyIntensities_df)) #0 duplicates

# IDs
unique(hourlyIntensities_df$Id) 
print(length(unique(hourlyIntensities_df$Id))) #IDs = 34
```
    
  The results show that this data set has four columns with the variables ID, ActivityHour, Totalintensity and AverageIntensity. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  
  
        2.2.2.4 hourlySteps_df  
    
```{r}
# Analize the structure
head(hourlySteps_df, 3) # 3 columns, has IDs  
str(hourlySteps_df) 

#The duplicates 
sum(duplicated(hourlySteps_df)) #0 duplicates

# IDs
unique(hourlySteps_df$Id) 
print(length(unique(hourlySteps_df$Id))) #IDs = 34
```

  The results show that this data set has three columns with the variables ID, ActivityHour and StepTotal. Furthermore, it has zero duplicates, and the number of unique IDs is 34.    
    
        2.2.2.5 minuteCaloriesNarrow_df  

```{r}
# Analize the structure
head(minuteCaloriesNarrow_df, 3) # 3 columns, has IDs  
str(minuteCaloriesNarrow_df) 

#The duplicates 
sum(duplicated(minuteCaloriesNarrow_df)) #0 duplicates

# IDs
unique(minuteCaloriesNarrow_df$Id) 
print(length(unique(minuteCaloriesNarrow_df$Id))) #IDs = 34
```

  The results show that this data set has three columns with the variables ID, ActivityMinute and Calories. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  

        2.2.2.6 minuteIntensitiesNarrow_df  

```{r}
# Analize the structure
head(minuteIntensitiesNarrow_df, 3) # 3 columns, has IDs  
str(minuteIntensitiesNarrow_df) 

#The duplicates 
sum(duplicated(minuteIntensitiesNarrow_df)) #0 duplicates

# IDs
unique(minuteIntensitiesNarrow_df$Id) 
print(length(unique(minuteIntensitiesNarrow_df$Id))) #IDs = 34
```

  The results show that this data set has three columns with the variables ID, ActivityMinute and Intensity. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  

        2.2.2.7 minuteMETsNarrow_df  
    
```{r}
# Analize the structure
head(minuteMETsNarrow_df, 3) # 3 columns, has IDs  
str(minuteMETsNarrow_df) 

#The duplicates 
sum(duplicated(minuteMETsNarrow_df)) #0 duplicates

# IDs
unique(minuteMETsNarrow_df$Id) 
print(length(unique(minuteMETsNarrow_df$Id))) #IDs = 34
```

  The results show that this data set has three columns with the variables ID, ActivityMinute and METs. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  

        2.2.2.8 minuteSleep_df  

```{r}
# Analize the structure
head(minuteSleep_df, 3) # 4 columns, has IDs  
str(minuteSleep_df) 

#The duplicates 
sum(duplicated(minuteSleep_df)) #525 duplicates

# IDs
unique(minuteSleep_df$Id) 
print(length(unique(minuteSleep_df$Id))) #IDs = 23
```

  The results show that this data set has four columns with the variables ID, date, value and logId. Furthermore, it has 525 duplicates, and the number of unique IDs is 23.  

          2.2.2.8.1 Analyze the duplicates and remove them from the file.   

```{r}
#To see if there are registers from the same ID in the same date duplicated:
minuteSleep_df %>%
  group_by(Id, date) %>%
  filter(n() > 1) %>%
  arrange(Id,date)

#We see the duplicates so it is time to remove them
minuteSleep_df <- minuteSleep_df %>% distinct()

# To confirm the number of duplicates  
sum(duplicated(minuteSleep_df)) #0
```

In the file we obtain 23 unique IDs.  

        2.2.2.9 minuteStepsNarrow_df  
    
```{r}
# Analize the structure
head(minuteStepsNarrow_df, 3) # 3 columns, has IDs  
str(minuteStepsNarrow_df) 

#The duplicates 
sum(duplicated(minuteStepsNarrow_df)) #0 duplicates

# IDs
unique(minuteStepsNarrow_df$Id) 
print(length(unique(minuteStepsNarrow_df$Id))) #IDs = 34
```

  The results show that this data set has three columns with the variables ID, ActivityMinute and Steps. Furthermore, it has zero duplicates, and the number of unique IDs is 34.  

        2.2.2.10 weightLogInfo_df  
    
```{r}
# Analize the structure
head(weightLogInfo_df, 3) # 8 columns, has IDs  
str(weightLogInfo_df) 

#The duplicates 
sum(duplicated(weightLogInfo_df)) #0 duplicates

# IDs
unique(weightLogInfo_df$Id) 
print(length(unique(weightLogInfo_df$Id))) #IDs = 11
```

  The results show that this data set has eight columns with the variables ID, Date, WeightKg, WeightPounds, Fat, BMI, IsManualReport and LogId. Furthermore, it has zero duplicates, and the number of unique IDs is 11. 
  

#### 3. Compare and analyse the IDs from each file in Excel 

Most of the sample size of the other data set is 34. So I will analyze the extra ID user (4388161847) from Daily_Activity_df to check if it was an error, and I can remove it.  

##### 3.1 Filter the lines of this ID user.
```{r}
filas_seleccionadas <- Daily_Activity_df %>%
  filter(Id == "4388161847")
print(filas_seleccionadas) # Shows the 8 rows 
```
    
  It shows that the only minimum registered information is on eight registers and is on sedentary time and calories. The user 4388161847 has only 8 observations and doesn't have a lot of information, so I will remove it.  

##### 3.2 Remove the Id from the data and confirm the action.

```{r}
Daily_Activity_df <- Daily_Activity_df %>%
  filter(Id != "4388161847")

str(Daily_Activity_df) # 499 obs.

print(length(unique(Daily_Activity_df$Id))) #34 IDs
```

#### 4. Change the ActivityDate format (chr) to format date and confirm.  

```{r}
Daily_Activity_df <- Daily_Activity_df %>% 
  mutate(ActivityDate = as.Date(ActivityDate, format = "%m/%d/%Y")) # It fixed the format 

str(Daily_Activity_df) #to confirm the format
```

The format has change and appears as 2016-03-25.     

#### 5. Make the operations to see if the values in each column are right.  

##### 5.1 Distances 

###### 5.1.1 Tracker distance

Tracker distance = Very Active distance + Moderate Active distance + Light Active distance

```{r}
Daily_Activity_df <- Daily_Activity_df %>%
  mutate(Resultado_T_V_M_L = TrackerDistance - (VeryActiveDistance + ModeratelyActiveDistance + LightActiveDistance) )

Daily_Activity_df$Resultado_T_V_M_L # Analyze the differences because we need the result to around 0

summary(Daily_Activity_df$Resultado_T_V_M_L) # we can see that the difference are strange so we have a max of 7.41 and min -6.41
```

We calculated the difference between the tracker distance and the sum of the activity levels per distance, which should give a result of around 0. However, this is not the case for all records, so their minimum and maximum values are -6.41 and 7.41. The results show a big range different from 0.

        5.1.1.1  Analyze the outliers shown in the formula applied above, T-(V-M-L)=0, using Tukey's rule. With the result, we create another dataframe to enter those records that give the outlier value and analyze them.
        
Tukey's rule

  This is a statistical method that allows outliers in a dataset to be identified and lets us make a decision on whether the results are significantly different or not (Tukey, 1977).


```{r}
Q1 <- quantile(Daily_Activity_df$Resultado_T_V_M_L, 0.25, na.rm = TRUE)
Q3 <- quantile(Daily_Activity_df$Resultado_T_V_M_L, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1 #0.0099

outliers_df <- Daily_Activity_df %>%
  filter(Resultado_T_V_M_L < (Q1 - 1.5 * IQR) |
           Resultado_T_V_M_L > (Q3 + 1.5 * IQR))
View(outliers_df) #  70 atypical values
```

        5.1.1.2. Graph to see how the data is distributed.  
        
Create a histogram to see if the outliers are very isolated.    

```{r}
ggplot(Daily_Activity_df, aes(x = Resultado_T_V_M_L)) +
  geom_histogram(bins = 50) +
  geom_vline(aes(xintercept = Q1), color = "blue", linetype = "dashed") +
  geom_vline(aes(xintercept = Q3), color = "blue", linetype = "dashed") +
  geom_vline(aes(xintercept = Q1 - 1.5*IQR), color = "red") +
  geom_vline(aes(xintercept = Q3 + 1.5*IQR), color = "red") +
  labs(title =  "Distribution of de diference of Tracker distance",size= 1.5, subtitle = "T-(V+M+L)")+
  labs(x = "Results of the diference", y = "Amount of registers") 
```

        5.1.1.3.Comparison of means between df.
        
```{r}
mean(Daily_Activity_df$Resultado_T_V_M_L) #0.06118
mean(outliers_df$Resultado_T_V_M_L) #0.38342
```

  If we compare the mean between the two, Daily_Activity_df and outlier_df, the expected results should be similar. But it can be seen that the outliers tend toward positive values and are much larger than the results in the original df, which can cause errors in subsequent analyses. Therefore, it is decided to treat them.

        5.1.1.4 Analyze registers of the users that have the biggest or lowest values in the outliers_df.    

```{r}
print(outliers_df %>%
  group_by(Id) %>%
  summarise(
    media = mean(Resultado_T_V_M_L),
    maximo = max(Resultado_T_V_M_L),
    minimo = min(Resultado_T_V_M_L),
    n = n()
  ) %>%
  arrange(desc(abs(media))), n=Inf) # there are 17 users with outliers
```

          5.1.1.4.1 View the registers of the 5 principal users that generate atypical values.   
        
```{r, echo=TRUE, results='hide'}
outliers_df %>%
  filter(Id %in% c(6391747486, 2891001357, 4020332650, 6962181067,7007744171)) %>%
  select(Id, ActivityDate, TrackerDistance, VeryActiveDistance,
         ModeratelyActiveDistance, LightActiveDistance, Resultado_T_V_M_L) %>%
  arrange(Id, ActivityDate, n= Inf) #  There are 40 registers of this users with outliers. 
```

Because there are 40 rows, i hide the result.  

          5.1.1.4.2 Analyze the registers of the first ID (6391747486).  

```{r}
#6391747486
Filas_seleccionadas<- outliers_df %>%
  filter(Id == "6391747486")
print(Filas_seleccionadas)
```

Two records are observed on different dates for this ID; the first shows that the Tracker distance is zero but has a Very Active distance of 2.03 miles. These may mean that there is a device synchronization error or a recording failure. It should be deleted because it breaks the logic of the variables, distorts the average and range, and may generate errors in subsequent analyses.  

          5.1.1.4.3 Search both data frames for how many records have TrackerDistance equal to zero and activity levels measured in distance greater than zero.

Trd = 0 & (V >0 | M >0 |L > 0 | S > 0)

```{r}
Daily_Activity_df %>%
  filter(TrackerDistance == 0 & (VeryActiveDistance > 0 | ModeratelyActiveDistance > 0 | LightActiveDistance > 0 | SedentaryActiveDistance > 0))
```

It was obtained 3 registers, two from the ID 2891001357 and one from the ID 6391747486.   

          5.1.1.4.4 Remove the registers because it is impossible that they mark and confirm the action.  

```{r}

#remove
Daily_Activity_df <- Daily_Activity_df %>%
  filter(!(TrackerDistance == 0 & 
             (VeryActiveDistance > 0 | ModeratelyActiveDistance > 0 | LightActiveDistance > 0 | SedentaryActiveDistance > 0)))

outliers_df <- outliers_df %>%
  filter(!(TrackerDistance == 0 & 
             (VeryActiveDistance > 0 | ModeratelyActiveDistance > 0 | LightActiveDistance > 0 | SedentaryActiveDistance > 0)))
# Confirm

Daily_Activity_df %>%
  filter(TrackerDistance == 0 & (VeryActiveDistance > 0 | ModeratelyActiveDistance > 0 | LightActiveDistance > 0 | SedentaryActiveDistance > 0)) 
# 0 obs.

str(Daily_Activity_df) # now 446 obs.
print(length(unique(Daily_Activity_df$Id))) # 34 IDs

str(outliers_df) # now 67 obs. 
print(length(unique(outliers_df$Id))) #  16 IDs it was remove one user

```

The Daily_Activity_df shows now 446 observations and outliers_df shows 67 observations with out that registers.   

          5.1.1.4.5. Analyze the other register from the ID (6391747486).  

```{r}
Filas_seleccionadas<- outliers_df %>%
  filter(Id == "6391747486")
print(Filas_seleccionadas)
```

The tracker value is much smaller than the distance of each activity at different levels. Also, it shows a value of 7.46 miles in fairly distance, which is physically impossible. This result can be attributed to a Fitbit synchronization error, a recording error caused by using two devices at the same time, causing the information to overlap, or an error in the export and transformation of data.   

So, to remedy this, we will look at how many records have a tracker value less than the sum of the activity levels. It is important to consider the IQR of 0.009 that we calculated above with Tukey's rule to get the upper and lower limits.  

          5.1.1.4.6. Calculate the upper and lower limits, see how many records have that error, and analyze a row to check if it has the error and remove.
        
```{r}
#upper and lower limits
lower_bound <- Q1 - 1.5 * IQR # 25% -0.014999
upper_bound <- Q3 + 1.5 * IQR # 75% 0.024999

#Se how many registers have this error.
outliers_detected <- outliers_df %>%
  filter(Resultado_T_V_M_L < lower_bound | Resultado_T_V_M_L > upper_bound) 
# There are 67. registers. 

#View(outliers_detected) # these are the registers in df outliners

# confirm with an example
Filas_seleccionadas<- outliers_df %>%
  filter(Id == "1503960366")
print(Filas_seleccionadas) 

#Remove outliers that exceed the limits of the dataframe outliers_df to see how many would remain and what problem they have.

outliers_df <- outliers_df[outliers_df$Resultado_T_V_M_L >= lower_bound & outliers_df$Resultado_T_V_M_L <= upper_bound, ]

str(outliers_df) # now 0 obs.

Daily_Activity_df <- Daily_Activity_df[Daily_Activity_df$Resultado_T_V_M_L >= lower_bound & Daily_Activity_df$Resultado_T_V_M_L <= upper_bound, ]
str(Daily_Activity_df) #379 obs.
print(length(unique(Daily_Activity_df$Id))) #34 IDs

```

The outlier_df now has 0 obs. and Daily_Activity_df has 379 observations of 34 users.   

          5.1.1.4.7 Make a histogram with the results.  
        
```{r}
ggplot(Daily_Activity_df, aes(x = Resultado_T_V_M_L)) +
  geom_histogram(bins = 10) +
  geom_vline(aes(xintercept = Q1), color = "blue", linetype = "dashed") +
  geom_vline(aes(xintercept = Q3), color = "blue", linetype = "dashed") +
  geom_vline(aes(xintercept = Q1 - 1.5*IQR), color = "red") +
  geom_vline(aes(xintercept = Q3 + 1.5*IQR), color = "red") +
  labs(title =  "Distribution of de diference of Tracker distance", subtitle = "T-(V+M+L)")+
  labs(x = "Results of the diference", y = "Amount of registers") +
  theme(axis.title.x = element_text(size = 8),axis.title.y =element_text(size = 8),axis.title = element_text(size=10))

```

###### 5.1.2 Total distance  

        5.1.2.1 Total distance - (Tracker distance + Logged distance) ~ 0  

T-(Tr+L)=0 

```{r}
Daily_Activity_df <- Daily_Activity_df %>%
  mutate(Resultado_T_T_L = TotalDistance -(TrackerDistance + LoggedActivitiesDistance) )

# Daily_Activity_df$Resultado_T_T_L # Analyze the differences because we need the result to around 0

summary(Daily_Activity_df$Resultado_T_T_L)  #Min -4.82803, Max 0
```

The minimum value of the difference is -4.82803, and the maximum is zero. So it is necessary to view and analyze the atypical values that tend to negative numbers. 

        5.1.2.2 Use the Tukey rule to obtain the atypical values, analyze them, and remove them if it is necessary.    
```{r}
Q1 <- quantile(Daily_Activity_df$Resultado_T_T_L, 0.25, na.rm = TRUE) # 25% 0
Q3 <- quantile(Daily_Activity_df$Resultado_T_T_L, 0.75, na.rm = TRUE) # 75% 0
IQR <- Q3 - Q1 #Tukey' Rule 0 

lower_bound <- Q1 - 1.5 * IQR # 25% 0
upper_bound <- Q3 + 1.5 * IQR # 75% 0

outliers_df <- Daily_Activity_df %>%
  filter(Resultado_T_T_L < (Q1 - 1.5 * IQR) |
           Resultado_T_T_L> (Q3 + 1.5 * IQR))

View(outliers_df)  

```


```{r}
# Remove 

outliers_df <- outliers_df[outliers_df$Resultado_T_T_L >= lower_bound & outliers_df$Resultado_T_T_L <= upper_bound, ]
Daily_Activity_df <- Daily_Activity_df[Daily_Activity_df$Resultado_T_T_L >= lower_bound & Daily_Activity_df$Resultado_T_T_L <= upper_bound, ]

# confirm
#View(outliers_df)  # there is nothing
str(Daily_Activity_df) #371 obs
print(length(unique(Daily_Activity_df$Id))) #34 users
```


There are eight registers with atypical values; seven of them are from the user 837856200, and one of the registers is from the user 2891001357. After removing the atypical values, we remain with 371 obs. and 34 users in the Daily_Activity_df and 0 obs. in the outliers_df.    

##### 5.2 Zero calories is an error.  

```{r}
#How many registers have zero calories.
sum(Daily_Activity_df$Calories == 0) #5

# Get the percentage of these cases against all the values in the data frame.

mean(Daily_Activity_df$Calories == 0) * 100 #1.37741%

Daily_Activity_df %>%
  filter(Calories == 0) # In this 5 registers we can see that the only value is 1440 sedentary minutes, the other variables are in zero.

# Remove this cases 
Daily_Activity_df <- Daily_Activity_df %>%
  filter(!(Calories == 0))
# confirm
str(Daily_Activity_df) #366 obs. 
print(length(unique(Daily_Activity_df$Id))) #34 users
```

Daily_Activity_df had only five cases in which the users had 0 calories; we removed them because it was an error.  

##### 5.3  EXTRA  
I checked the ones with 0 total steps because it seemed strange to me that some would have registered minutes of activity. After searching, I found out that the smart devices of Fitbit register the activity minutes with the heart rate of the client, not with the steps. So it is possible to have zero steps and minutes of activity.  

```{r}
Daily_Activity_df %>%
  filter(TotalSteps == 0) 
```

The two cases that showed very active minutes (33 and 20), but zero total steps or distance, were kept because they could be exercises such as Pilates or weight training.  

##### 5.4 Analyze the relation between total distance and steps when one of them is bigger than zero and the other is zero.  

```{r}
Daily_Activity_df %>%
  filter((TotalSteps == 0 & TotalDistance > 0) | (TotalSteps > 0 & TotalDistance == 0) ) # maintain the 2 rows 
```

Both registers were maintained.  

##### 5.5 Analyze the statistics values of each column at this point.  

```{r}
summary(Daily_Activity_df$TotalSteps)
summary(Daily_Activity_df$TotalDistance)
summary(Daily_Activity_df$TrackerDistance)
summary(Daily_Activity_df$LoggedActivitiesDistance)

Daily_Activity_df %>%
  filter(LoggedActivitiesDistance >0) # gives zero as value

summary(Daily_Activity_df$VeryActiveDistance)
summary(Daily_Activity_df$ModeratelyActiveDistance)
summary(Daily_Activity_df$LightActiveDistance)
summary(Daily_Activity_df$SedentaryActiveDistance)
summary(Daily_Activity_df$VeryActiveMinutes)
summary(Daily_Activity_df$FairlyActiveMinutes)
summary(Daily_Activity_df$LightlyActiveMinutes)
summary(Daily_Activity_df$SedentaryMinutes)
summary(Daily_Activity_df$Calories) # Here we have problems that we will solve after checking the minutes values.
```

The next step is analyze the minutes formulas and the we will check the errors in the calories values.  

##### 5.6 Minutes
It is known that the sum of VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes + SedentaryMinutes = 1440 minutes. So, if we apply 1440 - (VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes + SedentaryMinutes), it must be around zero minutes.  

```{r}
Daily_Activity_df_raw <- Daily_Activity_df %>%
  mutate(
    Minutos_registrados = VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes + SedentaryMinutes,
    Minutos_no_registrados = 1440 - Minutos_registrados
  )
```

###### 5.6.1 Analyze the use of the smart device before removing values.  

We apply the Tukey law to remove days with atypical values and create a CSV that have the data before removing atypical values for future analysis.      

```{r}

# Tukey rule to remove outliers in total minutes
Q1 <- quantile(Daily_Activity_df_raw$Minutos_no_registrados, 0.25, na.rm = TRUE)
Q3 <- quantile(Daily_Activity_df_raw$Minutos_no_registrados, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

lower <- Q1 - 1.5 * IQR
upper <- Q3 + 1.5 * IQR

Daily_Activity_df_raw <- Daily_Activity_df_raw %>% 
  filter(Minutos_no_registrados >= lower & Minutos_no_registrados <= upper)


str(Daily_Activity_df_raw) #358 obs
print(length(unique(Daily_Activity_df_raw$Id))) #34
str(Daily_Activity_df) #366
print(length(unique(Daily_Activity_df$Id))) #34

# Save in csv file the data that does not filter the observations without use for later analysis.

write.csv(Daily_Activity_df, "C:/Proyecto Google/Prepare/Analysis/Daily_Activity_without_filter_of_minutes.csv", row.names = FALSE)
# change the data from Daily_Activity_df_raw to Daily_Activity_df so that we have all the filters.

Daily_Activity_df <- Daily_Activity_df_raw
```

##### 5.7 Calories 

For this section, it was research the daily average energy expenditure (calories) of an adult woman. It is assumed that the tracker includes each user's metabolic rate (BMR) to calculate daily calorie expenditure. It is estimated that a healthy adult woman burns a minimum of 1200 kcal per day (Mifflin & St Jeor, 1990; Frankenfield et al., 2005). However, considering that not all women have the same characteristics, 1000 kcal was considered the lower limit for calories burned.  

So we analyze the statistical values of the calories to understand our data.   

```{r}
summary(Daily_Activity_df$Calories)  
```

The results show a minimum value of 399 that is lower than the limit that we decide to select. So, we will analyze the records that have values lower than our minimum limit to see what happen.  

```{r}
outliers_df <- Daily_Activity_df %>%
  filter(Calories <= 1000)

View(outliers_df) #14 obs.
print(length(unique(outliers_df$Id))) # 14 users
```

Also, the means of the atypical values and all the values in the Daily_Activity_df were evaluated to see if the difference was too big.  

```{r}
summary(outliers_df$Calories)
#Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#399.0   553.5   725.0   666.0   776.5   938.0 

mean(Daily_Activity_df$Calories) #2106.047
```

Daily_Activity_df calories show a result of 2106.047, and the outliers_df calories result is 666; this means that the atypical values must be treated.  

```{r}
#Remove the atypical values

outliers_df <- outliers_df %>%
  filter(!Calories <= 1000)

Daily_Activity_df <- Daily_Activity_df %>%
  filter(!Calories <= 1000) 

#Confirm
#View(outliers_df) #0 obs.
str(Daily_Activity_df) #344 obs. 
print(length(unique(Daily_Activity_df$Id))) #33 users
```

##### 6. Remove extra columns that I make and maintaining the correct values. Then make a new CSV ready to process.  

```{r}
Daily_Activity_df <- Daily_Activity_df %>% #remove extra columns
  select(- Resultado_T_V_M_L) %>%
  select(- Resultado_T_T_L) %>%
  select(- Minutos_no_registrados ) %>%
  select(- Minutos_registrados)

head(Daily_Activity_df) #Check the values
str(Daily_Activity_df) #Check all the columns 344 obs.

# Create a new document with the changes.
Daily_Activity_ready_for_analysis <- Daily_Activity_df
write.csv(Daily_Activity_ready_for_analysis, "C:/Proyecto Google/Prepare/Analysis/Daily_Activity_ready_for_analysis.csv", row.names = FALSE)
```


Bibliography:

Tukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.  

Mifflin, M. D., St Jeor, S. T., Hill, L. A., Scott, B. J., Daugherty, S. A., & Koh, Y. O. (1990). A new predictive equation for resting energy expenditure in healthy individuals. The American Journal of Clinical Nutrition, 51(2), 241–247. https://doi.org/10.1093/ajcn/51.2.241. PubMed    

Frankenfield, D., Roth-Yousey, L., & Compher, C. (2005). Comparison of predictive equations for resting metabolic rate in healthy nonobese and obese adults: A systematic review. Journal of the American Dietetic Association, 105(5), 775–789. https://doi.org/10.1016/j.jada.2005.02.075. jandonline.org  